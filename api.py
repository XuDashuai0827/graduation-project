import csv
import requests
import time
import base64

# è®¾ç½®è®¿é—®ä»¤ç‰Œå’Œæœç´¢å…³é”®è¯
access_token = '' # github ä¸Šæ‰¾è‡ªå·±çš„ token
search_keyword = 'stars:2000..2003' # !!!!!!!ä¸€è½®
per_page = 100  # æ¯é¡µè¿”å›çš„ä»“åº“æ•°é‡
total_repositories_limit = 100000  # è¦è·å–çš„ä»“åº“æ•°é‡ä¸Šé™
request_delay = 2  # è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰

# åˆå§‹åŒ–å˜é‡
page = 1 # !!!!!!!!!!
total_repositories = []

# å‘èµ·å¤šä¸ª API è¯·æ±‚ï¼Œè·å–ä»“åº“æ•°æ®
while len(total_repositories) < total_repositories_limit:
    # æ„å»º API è¯·æ±‚çš„ URL
    url = f'https://api.github.com/search/repositories?q={search_keyword}&sort=stars&order=desc&per_page={per_page}&page={page}'

    # æ·»åŠ è¯·æ±‚å¤´éƒ¨ï¼ŒåŒ…æ‹¬è®¿é—®ä»¤ç‰Œ
    headers = {
        'Authorization': f'token {access_token}',
        'Accept': 'application/vnd.github.v3+json'
    }

    # å‘èµ· API è¯·æ±‚
    response = requests.get(url, headers=headers)

    # æ£€æŸ¥å“åº”çŠ¶æ€ç 
    if response.status_code == 200:
        print(f'************** EPOCH â€”â€” star arrange {search_keyword}, page {page} ****************')
        # è§£æå“åº”æ•°æ®
        data = response.json()
        repositories = data['items']
        print(len(repositories))
        for repo in repositories:
            full_name = repo['full_name']
            print(full_name)
            url = f"https://api.github.com/repos/{full_name}/readme"

            # è·å–è¯·æ±‚è¯¥ url çš„ç»“æœï¼Œå¹¶è½¬æ¢ä¸º json
            result = requests.get(url, headers=headers)
            readme_data = result.json()
            
            if 'message' in readme_data.keys() and readme_data['message']=="Not Found":
                print("____________notfound____________")
                repository_ids = [[repo['id'],repo['name'],repo['description'],'',repo['stargazers_count'],repo['language'],repo['topics']]]
                total_repositories.extend(repository_ids)
                continue
            
            if ('content' not in readme_data.keys()) and ('message' not in readme_data.keys()):
                print('ğŸ·Notice!!! This project has moved!!!')
                repository_ids = [[repo['id'],repo['name'],repo['description'],'',repo['stargazers_count'],repo['language'],repo['topics']]]
                total_repositories.extend(repository_ids)
                continue

            # ç”±äº content å†…å®¹æ˜¯ base64 ç¼–ç è¿‡çš„ï¼Œæ‰€ä»¥éœ€è¦å…ˆä½œè§£ç å¤„ç†ï¼Œä¸ç„¶è¿”å›çš„æ˜¯ä¸€å †å­—æ¯
            # è·å–å½“å‰é¡µçš„ä»“åº“ä¿¡æ¯
            repository_ids = [[repo['id'],repo['name'],repo['description'],base64.b64decode(readme_data['content']),repo['stargazers_count'],repo['language'],repo['topics']]]
            total_repositories.extend(repository_ids)
            
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°äº†è¦è·å–çš„ä»“åº“æ•°é‡ä¸Šé™
        if len(total_repositories) >= total_repositories_limit:
            break

        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
        if len(repositories) < per_page:
            break  # å·²ç»è·å–äº†æ‰€æœ‰ä»“åº“ï¼Œé€€å‡ºå¾ªç¯
        else:
            page+=1
            #if page<7:
            #    page += 1  # ç»§ç»­è·å–ä¸‹ä¸€é¡µ
            #else:
            #    break

        # å»¶è¿Ÿæš‚åœ
        time.sleep(request_delay)
        
    else:
        print(f"Request failed with status code {response.status_code}")
        break

# å°†ä»“åº“ä¿¡æ¯å†™å…¥ CSV æ–‡ä»¶
csv_filename = 'repos_data_2000.csv'
with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:
    writer = csv.writer(csvfile)
    #writer.writerow(['ID','name','description','readme','stars','language','topics'])
    writer.writerows([repo_id for repo_id in total_repositories])

print(f"Data saved to {csv_filename} successfully.")