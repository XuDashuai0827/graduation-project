{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import time\n",
    "import base64\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取CSV文件\n",
    "data = pd.read_csv('repos_data_2000.csv')\n",
    "\n",
    "print(data.tail(10))\n",
    "data_len = len(data)\n",
    "print(\"初始数据量为：\", data_len)\n",
    "# 去掉无标签数据行\n",
    "no_topic_list = []\n",
    "for i in range(data_len):\n",
    "    if data['topics'][i] == '[]':\n",
    "        no_topic_list.append(i)\n",
    "        #print(data['ID'][i],data['name'][i],data['topics'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('无标签数据量为：', len(no_topic_list))\n",
    "data_new=data.drop(no_topic_list) # 删除 no_topic_list 行数据\n",
    "data_new.to_csv(\"./data_new1.csv\",index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取CSV文件\n",
    "data_new1 = pd.read_csv('data_new1.csv')\n",
    "\n",
    "print(data_new1.tail(10))\n",
    "data_len1 = len(data_new1)\n",
    "print(\"有标签数据量为：\", data_len1)\n",
    "# 去掉单一标签数据行\n",
    "one_topic_list = []\n",
    "for i in range(data_len1):\n",
    "    if len(eval(data_new1['topics'][i])) == 1:\n",
    "        one_topic_list.append(i)\n",
    "        #print(data_new1['ID'][i],data_new1['name'][i],data_new1['topics'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('单标签数据量为：', len(one_topic_list))\n",
    "data_new=data_new1.drop(one_topic_list) # 删除 one_topic_list 行数据\n",
    "data_new.to_csv(\"./data_new2.csv\",index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new2 = pd.read_csv('data_new2.csv')\n",
    "print(data_new2.tail(10))\n",
    "data_len2 = len(data_new2)\n",
    "print(\"多标签数据量为：\", data_len2)\n",
    "# 去掉 ID 重复数据行（每次换 search 条件时数据可能会有所增减）\n",
    "ID_list = []\n",
    "duplicate_list = []\n",
    "for i in range(data_len2):\n",
    "    if data_new2['ID'][i] in ID_list:\n",
    "        duplicate_list.append(i)\n",
    "        #print(data_new2['ID'][i],data_new2['name'][i],data_new2['topics'][i])\n",
    "    ID_list.append(data_new2['ID'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ID 重复数据量为：', len(duplicate_list))\n",
    "data_new=data_new2.drop(duplicate_list) # 删除 duplicate_list 行数据\n",
    "data_new.to_csv(\"./data_new3.csv\",index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new3 = pd.read_csv('data_new3.csv')\n",
    "data_len3 = len(data_new3)\n",
    "print(\"清洗后的数据量为：\", data_len3)\n",
    "# 提取每个仓库的标签，并另存为一个文件\n",
    "topics_list = data_new3['topics']\n",
    "print(topics_list)\n",
    "csv_filename = 'origin_topics.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topics\\n')\n",
    "    for i in range(data_len3):\n",
    "        f.write(\"\\\"\")\n",
    "        f.write(str(topics_list[i]))\n",
    "        f.write(\"\\\"\")\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_set = set()\n",
    "count = 0\n",
    "for i in range(data_len3):\n",
    "    topic_list = eval(topics_list[i])\n",
    "    for item in topic_list:\n",
    "        topic_set.add(item)\n",
    "        count +=1 # 每个仓库的初始标签没有重复项\n",
    "print('初始不同标签数量：', len(topic_set))\n",
    "print('标签总数：', count)\n",
    "print('初始各标签平均出现频率：', round(count/len(topic_set),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标签统一化（实体消歧）\n",
    "import json\n",
    "\n",
    "file = open(\"rules/annotated_github_topics_wikidata.json\", 'r', encoding='utf-8')\n",
    "wiki = []\n",
    "for line in file.readlines():\n",
    "    dic = json.loads(line)\n",
    "    wiki.append(dic)\n",
    "wiki_dict = eval(str(wiki))\n",
    "#print(wiki[1])\n",
    "print('GitHub 标题的 wiki 百科大小为：',len(wiki_dict))\n",
    "#print(wiki_dict[1]['Wikidata Title']) # 字符串类型（只有一个）\n",
    "#print(wiki_dict[1]['GitHub Topic']) # list 类型\n",
    "#print(wiki_dict[1]['Wikidata Aliases']) # list 类型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据截选\n",
    "total_topic = []\n",
    "\n",
    "for i in range(data_len3):\n",
    "    topic_list = eval(topics_list[i])\n",
    "    repo_topic = []\n",
    "    for item in topic_list: # 对第 i+1 个仓库的每个标签\n",
    "        for j in range(len(wiki_dict)):\n",
    "            wiki_j = wiki_dict[j]['GitHub Topic'] + [wiki_dict[j]['Wikidata Title']] + wiki_dict[1]['Wikidata Aliases']\n",
    "            if item in wiki_j:\n",
    "                #print('第',i+1,'个仓库的主题标签',item,'为别称，替换为',wiki_j[0])\n",
    "                item = wiki_j[0] # 将 item 视为别称，并直接规范为 GitHub Topic 的第一个\n",
    "                break\n",
    "        repo_topic.append(item) \n",
    "    repo_topic = list(set(repo_topic))\n",
    "    total_topic.append(repo_topic)   \n",
    "#print(total_topic[3])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_set1 = set()\n",
    "count1 = 0\n",
    "for i in range(data_len3):\n",
    "    topic_list = set(total_topic[i])\n",
    "    topic_set1 = topic_set1.union(topic_list)\n",
    "    count1 += len(topic_list)\n",
    "print('wiki 处理后不同标签数量：', len(topic_set1))\n",
    "print('wiki 处理后标签总数：', count1)\n",
    "print('wiki 处理后各标签平均出现频率：', round(count1/len(topic_set1),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储经 wiki 百科资料处理过的主题标签数据\n",
    "csv_filename = 'topics1.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topics\\n')\n",
    "    for i in range(data_len3):\n",
    "        f.write(\"\\\"\")\n",
    "        f.write(str(total_topic[i]))\n",
    "        f.write(\"\\\"\")\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_csv = pd.read_csv('topics1.csv')\n",
    "topic_csv_len = len(topic_csv)\n",
    "print(\"主题数据量为：\", topic_csv_len)\n",
    "topics_list = topic_csv['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除版本号\n",
    "rule1 = pd.read_csv('rules/topics_contains_version.csv')\n",
    "rule_len1 = len(rule1)\n",
    "print(\"该规则数据量为：\", rule_len1)\n",
    "total_topic1 = []\n",
    "for i in range(topic_csv_len):\n",
    "    new_topic_list = []\n",
    "    topic_list = eval(topics_list[i])\n",
    "    for item in topic_list:\n",
    "        flag = 0\n",
    "        for j in range(rule_len1):\n",
    "            if rule1['version'][j]==item: # 匹配上版本号\n",
    "                print(i,\"个主题列中，\",item,\"是一个版本号，需修改\")\n",
    "                if rule1['topic'][j]=='-1': # 是版本号但无法修改，保留item\n",
    "                    print(\"该版本号不做修改\")\n",
    "                    break\n",
    "                else:\n",
    "                    item = (rule1['topic'][j])\n",
    "                    print(\"修改为\", item)\n",
    "                    if item[0] == '[':\n",
    "                        flag = 1\n",
    "                        item = eval(item)\n",
    "                    break\n",
    "        if flag == 1:\n",
    "            new_topic_list.extend(item)\n",
    "            continue\n",
    "        new_topic_list.extend([item])\n",
    "    total_topic1.append(new_topic_list)\n",
    "#print(total_topic1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储经版本号处理过的主题标签数据\n",
    "csv_filename = 'topics2.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topics\\n')\n",
    "    for i in range(topic_csv_len):\n",
    "        f.write(\"\\\"\")\n",
    "        f.write(str(total_topic1[i]))\n",
    "        f.write(\"\\\"\")\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_csv = pd.read_csv('topics2.csv')\n",
    "topic_csv_len = len(topic_csv)\n",
    "print(\"主题数据量为：\", topic_csv_len)\n",
    "topics_list = topic_csv['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除标签中的数字\n",
    "rule2 = pd.read_csv('rules/topics_contains_number.csv')\n",
    "rule_len2 = len(rule2)\n",
    "print(\"该规则数据量为：\", rule_len2)\n",
    "total_topic2 = []\n",
    "for i in range(topic_csv_len):\n",
    "    new_topic_list = []\n",
    "    topic_list = eval(topics_list[i])\n",
    "    for item in topic_list:\n",
    "        #flag = 0\n",
    "        for j in range(rule_len2):\n",
    "            if rule2['number'][j]==item: # 匹配上\n",
    "                print(i,\"个主题列中，\",item,\"是一个带数字标签，需修改为\",rule2['topic'][j])\n",
    "                item = rule2['topic'][j]\n",
    "                break\n",
    "        new_topic_list.append(item)\n",
    "    total_topic2.append(new_topic_list)\n",
    "print(total_topic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储经数字去除处理过的主题标签数据\n",
    "csv_filename = 'topics3.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topics\\n')\n",
    "    for i in range(topic_csv_len):\n",
    "        f.write(\"\\\"\")\n",
    "        f.write(str(total_topic2[i]))\n",
    "        f.write(\"\\\"\")\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_csv = pd.read_csv('topics3.csv')\n",
    "topic_csv_len = len(topic_csv)\n",
    "print(\"主题数据量为：\", topic_csv_len)\n",
    "topics_list = topic_csv['topics']\n",
    "print(topics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉版本和数字处理后重复出现的标签\n",
    "topic_set3 = set()\n",
    "count3 = 0\n",
    "topic3 = []\n",
    "for i in range(topic_csv_len):\n",
    "    topic_list = set(eval(topics_list[i]))\n",
    "    topic3.append(list(topic_list))\n",
    "    topic_set3 = topic_set3.union(topic_list)\n",
    "    count3 += len(topic_list)\n",
    "print('version 和 number 处理后不同标签数量：', len(topic_set3))\n",
    "print('version 和 number 处理后标签总数：', count3)\n",
    "print('version 和 number 处理后各标签平均出现频率：', round(count3/len(topic_set3),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储去重处理过的主题标签数据\n",
    "csv_filename = 'topics4.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topics\\n')\n",
    "    for i in range(topic_csv_len):\n",
    "        f.write(\"\\\"\")\n",
    "        f.write(str(topic3[i]))\n",
    "        f.write(\"\\\"\")\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 修改 rules\n",
    "# 词汇清洗条件筛选\n",
    "rule1 = pd.read_csv('rules/delete.csv')\n",
    "#rule2 = pd.read_csv('rules/abstract.csv')\n",
    "rule_len1 = len(rule1)\n",
    "#rule_len2 = len(rule2)\n",
    "print(\"该规则数据量为：\", rule_len1)#,rule_len2)\n",
    "number = []\n",
    "#for i in range(rule_len2):\n",
    "    #for j in range(i+1,rule_len2):\n",
    "    #    if (rule2['topic'][i]==rule2['topic'][j]):# and (rule1['topics'][i] in list(rule2['topic'])):\n",
    "    #        number.append(i)\n",
    "    #        break\n",
    "#print(number)\n",
    "#rule_1 = rule2.iloc[number[10:]]\n",
    "#print(rule_1)\n",
    "#rule1=rule1.drop(number[:7]) # 删除 duplicate_list 行数据\n",
    "rule1[['del']].to_csv(\"rules/delete.csv\",index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 长标签分解关键词\n",
    "rule3 = pd.read_csv('rules/split_dash_topics.csv')\n",
    "rule_len3 = len(rule3)\n",
    "print(\"该规则数据量为：\", rule_len3)\n",
    "topic_csv = pd.read_csv('topics4.csv')\n",
    "topic_csv_len = len(topic_csv)\n",
    "print(\"主题数据量为：\", topic_csv_len)\n",
    "topics_list = topic_csv['topics']\n",
    "t_oreder = ['t1','t2','t3','t4','t5'] ###\n",
    "new_total_topic = []\n",
    "for i in range(topic_csv_len):\n",
    "    one_topic_list = eval(topics_list[i])\n",
    "    new_topic_list = []\n",
    "    for item in one_topic_list:\n",
    "        flag = 0\n",
    "        item_len = len(item.split('-'))\n",
    "        if (item_len)>1:\n",
    "            for j in range(rule_len3):\n",
    "                if item == rule3['total'][j]: # 找到，拆解，退出该词查找\n",
    "                    print(i,'仓库中的',item,\"需拆解\")\n",
    "                    re_item = []\n",
    "                    for k in range(5): ###\n",
    "                        if str(rule3[t_oreder[k]][j])!='nan':\n",
    "                            re_item.append(rule3[t_oreder[k]][j])\n",
    "                    #print(re_item)\n",
    "                    flag = 1\n",
    "                    new_topic_list.extend(re_item)\n",
    "                    break\n",
    "        if flag == 0:\n",
    "            new_topic_list.append(item)\n",
    "    no_same = set(new_topic_list)\n",
    "    new_total_topic.append(list(no_same))\n",
    "print(new_total_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标签失误处理：找出被错误处理的仓库id\n",
    "topic_csv = pd.read_csv('topics4.csv')\n",
    "topic_csv_len = len(topic_csv)\n",
    "print(\"主题数据量为：\", topic_csv_len)\n",
    "topics_list = topic_csv['topics']\n",
    "wrong_topic_id = []\n",
    "for i in range(topic_csv_len):\n",
    "    one_topic_list = eval(topics_list[i])\n",
    "    new_topic_list = []\n",
    "    for item in one_topic_list:       \n",
    "        divid = item.split('-')\n",
    "        item_len = len(divid)\n",
    "        if (item_len)>2:\n",
    "            if divid[2] == 'code' and divid[1] == 'studio' and divid[0]=='visual': # 错误拆解词\n",
    "                print(i,'仓库中的',item,\"需补充\")\n",
    "                wrong_topic_id.append(i)\n",
    "                break\n",
    "print(wrong_topic_id)\n",
    "print(len(wrong_topic_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标签失误处理：根据仓库id修正错误\n",
    "topic_csv = pd.read_csv('topics8.csv')\n",
    "topic_csv_len = len(topic_csv)\n",
    "print(\"主题数据量为：\", topic_csv_len)\n",
    "topics_list = topic_csv['topics']\n",
    "correct_topic = []\n",
    "count = 0\n",
    "for i in range(topic_csv_len):\n",
    "    one_topic_list = eval(topics_list[i])\n",
    "    new_topic_list = []\n",
    "    flag = 0\n",
    "    if i in wrong_topic_id:       \n",
    "        print(i,'仓库中的错误标签需添加')\n",
    "        one_topic_list.append('vscode') # 添加被错误删除的词\n",
    "        print(one_topic_list)\n",
    "        count += 1\n",
    "    correct_topic.append(one_topic_list)\n",
    "print(len(correct_topic))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储错误处理修改回来后的标签\n",
    "csv_filename = 'topics8.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topics\\n')\n",
    "    for i in range(topic_csv_len):\n",
    "        f.write(\"\\\"\")\n",
    "        f.write(str(correct_topic[i])) ###new_total_topic\n",
    "        f.write(\"\\\"\")\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对抽象和复数标签进行转换\n",
    "rule4 = pd.read_csv('rules/abstract_plural_topics.csv')\n",
    "rule_len4 = len(rule4)\n",
    "print(\"该规则数据量为：\", rule_len4)\n",
    "topic_csv = pd.read_csv('topics5.csv')\n",
    "topic_csv_len = len(topic_csv)\n",
    "print(\"主题数据量为：\", topic_csv_len)\n",
    "topics_list = topic_csv['topics']\n",
    "new_total_topic = []\n",
    "for i in range(topic_csv_len):\n",
    "    one_topic_list = eval(topics_list[i])\n",
    "    new_topic_list = []\n",
    "    for item in one_topic_list:\n",
    "        flag = 0\n",
    "        for j in range(rule_len4):\n",
    "            if item == rule4['abstract'][j]: # 找到，转换，退出该词查找\n",
    "                print(i,'仓库中的',item,\"需转换\")\n",
    "                new_topic_list.append(rule4['topic'][j])\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 0:\n",
    "            new_topic_list.append(item)\n",
    "    new_total_topic.append(list(set(new_topic_list)))\n",
    "print(new_total_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储抽象和复数标签转换过的主题标签数据\n",
    "csv_filename = 'topics6.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topics\\n')\n",
    "    for i in range(topic_csv_len):\n",
    "        f.write(\"\\\"\")\n",
    "        f.write(str(new_total_topic[i])) \n",
    "        f.write(\"\\\"\")\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除停用词\n",
    "rule5 = pd.read_csv('rules/remove_stopwords_topic.csv')\n",
    "rule_len5 = len(rule5)\n",
    "print(\"该规则数据量为：\", rule_len5)\n",
    "topic_csv = pd.read_csv('topics6.csv')\n",
    "topic_csv_len = len(topic_csv)\n",
    "print(\"主题数据量为：\", topic_csv_len)\n",
    "topics_list = topic_csv['topics']\n",
    "new_total_topic = []\n",
    "for i in range(topic_csv_len):\n",
    "    one_topic_list = eval(topics_list[i])\n",
    "    new_topic_list = []\n",
    "    for item in one_topic_list:\n",
    "        if item not in list(rule5['stop_word']):\n",
    "            new_topic_list.append(item)\n",
    "        else:\n",
    "            print(i,'仓库中的',item,\"被移除\")\n",
    "    new_total_topic.append(list(set(new_topic_list)))\n",
    "print(new_total_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储去除停用词后的主题标签数据\n",
    "csv_filename = 'topics7.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topics\\n')\n",
    "    for i in range(topic_csv_len):\n",
    "        f.write(\"\\\"\")\n",
    "        f.write(str(new_total_topic[i])) \n",
    "        f.write(\"\\\"\")\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除非标签词汇\n",
    "rule6 = pd.read_csv('rules/delete.csv')\n",
    "rule_len6 = len(rule6)\n",
    "print(\"该规则数据量为：\", rule_len6)\n",
    "topic_csv = pd.read_csv('topics7.csv')\n",
    "topic_csv_len = len(topic_csv)\n",
    "print(\"主题数据量为：\", topic_csv_len)\n",
    "topics_list = topic_csv['topics']\n",
    "new_total_topic = []\n",
    "for i in range(topic_csv_len):\n",
    "    one_topic_list = eval(topics_list[i])\n",
    "    new_topic_list = []\n",
    "    for item in one_topic_list:\n",
    "        if item not in list(rule6['del']):\n",
    "            new_topic_list.append(item)\n",
    "        else:\n",
    "            print(i,'仓库中的',item,\"被移除\")\n",
    "    new_total_topic.append(list(set(new_topic_list)))\n",
    "print(new_total_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储去除非标签词汇后的主题标签数据\n",
    "csv_filename = 'topics8.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topics\\n')\n",
    "    for i in range(topic_csv_len):\n",
    "        f.write(\"\\\"\")\n",
    "        f.write(str(new_total_topic[i])) \n",
    "        f.write(\"\\\"\")\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再次对标签进行wiki清洗\n",
    "import json\n",
    "file = open(\"rules/annotated_github_topics_wikidata.json\", 'r', encoding='utf-8')\n",
    "wiki = []\n",
    "for line in file.readlines():\n",
    "    dic = json.loads(line)\n",
    "    wiki.append(dic)\n",
    "wiki_dict = eval(str(wiki))\n",
    "print('GitHub 标题的 wiki 百科大小为：',len(wiki_dict))\n",
    "topic_csv = pd.read_csv('topics8.csv')\n",
    "topic_csv_len = len(topic_csv)\n",
    "print(\"主题数据量为：\", topic_csv_len)\n",
    "topics_list = topic_csv['topics']\n",
    "total_topic = []\n",
    "change = set()\n",
    "for i in range(topic_csv_len):\n",
    "    topic_list = eval(topics_list[i])\n",
    "    repo_topic = []\n",
    "    for item in topic_list: # 对第 i+1 个仓库的每个标签\n",
    "        for j in range(len(wiki_dict)):\n",
    "            wiki_j = wiki_dict[j]['GitHub Topic'] + [wiki_dict[j]['Wikidata Title']] + wiki_dict[1]['Wikidata Aliases']\n",
    "            if (item in wiki_j) and (item != wiki_j[0]):\n",
    "                print('第',i+1,'个仓库的主题标签',item,'为别称，替换为',wiki_j[0])\n",
    "                item = wiki_j[0] # 将 item 视为别称，并直接规范为 GitHub Topic 的第一个\n",
    "                change.add(item)\n",
    "                break\n",
    "        repo_topic.append(item) \n",
    "    repo_topic = list(set(repo_topic))\n",
    "    total_topic.append(repo_topic) \n",
    "print(change)\n",
    "print(total_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储wiki清洗后的主题标签数据\n",
    "csv_filename = 'topics9.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topics\\n')\n",
    "    for i in range(topic_csv_len):\n",
    "        f.write(\"\\\"\")\n",
    "        f.write(str(total_topic[i])) \n",
    "        f.write(\"\\\"\")\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各仓库标签去重\n",
    "topic_csv = pd.read_csv('topics4.csv')\n",
    "topic_csv_len = len(topic_csv)\n",
    "print(\"主题数据量为：\", topic_csv_len)\n",
    "topics_list = topic_csv['topics']\n",
    "new_total_topic = []\n",
    "for i in range(topic_csv_len):\n",
    "    one_topic_list = eval(topics_list[i])\n",
    "    new_total_topic.append(list(set(one_topic_list)))\n",
    "print(new_total_topic[1])\n",
    "csv_filename = 'topics4.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topics\\n')\n",
    "    for i in range(topic_csv_len):\n",
    "        f.write(\"\\\"\")\n",
    "        f.write(str(new_total_topic[i])) \n",
    "        f.write(\"\\\"\")\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
