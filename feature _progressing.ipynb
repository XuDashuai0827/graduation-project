{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取各仓库主题标签的第一个为代表\n",
    "topic_csv = pd.read_csv('final_topics.csv')\n",
    "topic_list = topic_csv['topics']\n",
    "topic_csv_len = len(topic_csv)\n",
    "main_topic_csv = pd.read_csv('main256_topic.csv')\n",
    "main_topic_list = main_topic_csv['topics']\n",
    "represent_topic_list = []\n",
    "max_len = 0 \n",
    "topic_len_list = dict()\n",
    "for i in range(topic_csv_len):\n",
    "    topic_len = len(eval(topic_list[i]))\n",
    "    if max_len < topic_len:\n",
    "        max_len = topic_len\n",
    "    if topic_len not in topic_len_list.keys():\n",
    "        topic_len_list.update({topic_len:1})\n",
    "    else:\n",
    "        topic_len_list[topic_len] +=1\n",
    "    represent_topic = eval(topic_list[i])[0]\n",
    "    count256 = 0\n",
    "    for j in range(256):\n",
    "        if represent_topic == main_topic_list[j]:\n",
    "            represent_topic_list.append(j)\n",
    "            break\n",
    "        count256 += 1\n",
    "        if count256 ==256:\n",
    "            print(represent_topic)\n",
    "print(represent_topic_list)\n",
    "print('各仓库最多标签数：',max_len)\n",
    "print(topic_len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取代表标签的位置并保存（空白对照组）\n",
    "print(len(represent_topic_list))\n",
    "def create_identity_matrix(n):\n",
    "    identity_matrix = [[0] * n for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        identity_matrix[i][i] = 1\n",
    "    return identity_matrix\n",
    "# 调用函数生成 256*256 的单位矩阵\n",
    "identity_feature = create_identity_matrix(256)\n",
    "print(identity_feature)\n",
    "represent_feature = []\n",
    "for i in represent_topic_list:\n",
    "    represent_feature.append(identity_feature[i])\n",
    "txt_filename = 'feature_vector/feature_without.txt' ##\n",
    "with open(txt_filename,'w',encoding='utf-8') as f:\n",
    "    f.write(str(represent_feature))\n",
    "    f.write('\\n')   \n",
    "print(f\"Data saved to {txt_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取代表标签的主题关联度特征并保存\n",
    "represent_feature = []\n",
    "txt_filename = 'topic_matrix/topic_matrix_collaborate.txt' ##\n",
    "with open(txt_filename,'r',encoding='utf-8') as f:\n",
    "    feature_matrix = eval(f.read())\n",
    "    for i in represent_topic_list:\n",
    "        represent_feature.append(feature_matrix[i])\n",
    "txt_filename = 'feature_vector/feature_collaborate.txt' ##\n",
    "with open(txt_filename,'w',encoding='utf-8') as f:\n",
    "    f.write(str(represent_feature))\n",
    "    f.write('\\n')   \n",
    "print(f\"Data saved to {txt_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-hot 编码处理仓库多标签\n",
    "topic_csv = pd.read_csv('final_topics.csv')\n",
    "topic_list = topic_csv['topics']\n",
    "topic_csv_len = len(topic_csv)\n",
    "main_topic_csv = pd.read_csv('main256_topic.csv')\n",
    "main_topic_list = main_topic_csv['topics']\n",
    "y = []\n",
    "for i in range(topic_csv_len):\n",
    "    one_y = [0]*256\n",
    "    one_topic_list = eval(topic_list[i])\n",
    "    one_len = len(one_topic_list)\n",
    "    for item in one_topic_list:\n",
    "        for j in range(256):\n",
    "            if item == main_topic_list[j]:\n",
    "                one_y[j] = 1\n",
    "                break\n",
    "    y.append(one_y)\n",
    "print(len(y),len(y[3]))\n",
    "txt_filename = 'y.txt'\n",
    "with open(txt_filename,'w',encoding='utf-8') as f:\n",
    "    f.write(str(y))\n",
    "print(f\"Labels saved to {txt_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先单独处理 readme 文件（转至 r_d_processing.ipynb 文件详细处理）\n",
    "readme_list = pd.read_csv('final_infors.csv')\n",
    "readme = readme_list['readme']\n",
    "readme.to_csv('observe/readme_origin.csv',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 翻译 \\x 编码\n",
    "from urllib import parse\n",
    "readme_csv = pd.read_csv('observe/readme_link4.csv')\n",
    "readme_list = readme_csv['readme']\n",
    "readme_csv_len = len(readme_list)\n",
    "print(\"仓库大小为：\",readme_csv_len)\n",
    "readme_k = [] # \n",
    "for i in range(readme_csv_len):\n",
    "    one_readme = readme_list[i][2:-1]\n",
    "    #print(one_readme)\n",
    "    one_readme_len = len(one_readme)\n",
    "    #print(one_readme_len)\n",
    "    s =  one_readme.encode('unicode_escape')\n",
    "    #print(s)\n",
    "    ss = s.decode('utf-8').replace('\\\\\\\\x', '%')\n",
    "    #print(ss)\n",
    "    new_readme = parse.unquote(ss)\n",
    "    #print(new_readme)\n",
    "    readme_k.append(new_readme) #\n",
    "    #print(len(new_readme)) \n",
    "print(len(readme_k)) #\n",
    "csv_filename = 'observe/readme_chinese.csv' #\n",
    "with open(csv_filename, 'a', encoding='utf-8') as f:\n",
    "    for i in range(len(readme_k)): #\n",
    "        f.write(\"\\\"\")\n",
    "        f.write(readme_k[i]) #\n",
    "        f.write(\"\\\"\")\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有要加入训练的数据合并（readme + 编程语言、仓库名称、description）\n",
    "info_csv = pd.read_csv('info_data.csv')\n",
    "info_csv_len = len(info_csv)\n",
    "print(\"description 仓库大小为：\",info_csv_len)\n",
    "with open('observe/readme_chinese.csv','r',encoding='utf-8') as f: # \n",
    "    readme_csv = f.readlines()\n",
    "print(\"文件加载完毕\")\n",
    "readme_list = readme_csv[1:]\n",
    "readme_csv_len = len(readme_list)\n",
    "print(\"readme 仓库大小为：\",readme_csv_len)\n",
    "csv_filename = 'd_r_data.csv' #\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"language,name,description,readme\\n\")\n",
    "    for i in range(info_csv_len): #\n",
    "        f.write(str(info_csv['language'][i]))\n",
    "        f.write(\",\")\n",
    "        f.write(str(info_csv['name'][i]))\n",
    "        f.write(\",\")\n",
    "        f.write(str(info_csv['description'][i]))\n",
    "        f.write(\",\")\n",
    "        f.write(readme_list[i])\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只保留英文和几个符号\n",
    "import re\n",
    "with open('d_r_data.csv','r',encoding='utf-8') as f: # \n",
    "    readme_csv = f.readlines()\n",
    "print(\"文件加载完毕\")\n",
    "readme_list = readme_csv[1:]\n",
    "readme_csv_len = len(readme_list)\n",
    "print(\"仓库大小为：\",readme_csv_len)\n",
    "#REPLACE_BY_SPACE_RE = re.compile('[/(){}[]|@,;.<>_《》]')\n",
    "english_list = []\n",
    "BAD_SYMBOLS_RE = re.compile('[^A-Z0-9a-z +.\\-\\\\\\\\\\\\\\']') # “0-9a-z”和“-.\\'+_”会被保留下来\n",
    "for i in range(readme_csv_len):\n",
    "    text =  readme_list[i]\n",
    "    text = BAD_SYMBOLS_RE.sub(' ',text)\n",
    "    print(text)\n",
    "    english_list.append(text)\n",
    "#text = REPLACE_BY_SPACE_RE.sub(' ',text)\n",
    "print(len(english_list))\n",
    "csv_filename = 'english_only.csv' #\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('language,name,description,readme\\n')\n",
    "    for i in range(len(english_list)): #\n",
    "        f.write(english_list[i])\n",
    "        f.write(\"\\n\")\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载停用词表格\n",
    "with open('rules/stopwords.txt', 'r') as f:\n",
    "    stopwords_list = f.readlines()\n",
    "stopwords_list = [i.strip() for i in stopwords_list]\n",
    "STOPWORDS = set(stopwords_list)\n",
    "print('停用词数量：', len(stopwords_list))\n",
    "print('停用词列表：',STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除停用词\n",
    "with open('english_only.csv','r',encoding='utf-8') as f: # \n",
    "    readme_csv = f.readlines()\n",
    "print(\"文件加载完毕\")\n",
    "readme_list = readme_csv[1:]\n",
    "readme_csv_len = len(readme_list)\n",
    "print(\"仓库大小为：\",readme_csv_len)\n",
    "readme_k = []\n",
    "k_count = 0\n",
    "for i in range(readme_csv_len):\n",
    "    text = readme_list[i]\n",
    "    text = text.lower()\n",
    "    s_text = ' '.join([w for w in text.split() if w not in STOPWORDS]) # 删除停用词\n",
    "    readme_k.append(s_text)\n",
    "print(len(readme_k))\n",
    "csv_filename = 'english_only3.csv' #\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('language,name,description,readme\\n')\n",
    "    for i in range(readme_csv_len): #\n",
    "        f.write(readme_k[i])\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据准备\n",
    "with open('english_only3.csv','r',encoding='utf-8') as f: # \n",
    "    readme_csv = f.readlines()\n",
    "print(\"文件加载完毕\")\n",
    "readme_list = readme_csv[1:]\n",
    "readme_csv_len = len(readme_list)\n",
    "print(\"仓库大小为：\",readme_csv_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文本文档转换为计数的稀疏矩阵\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_df=0.75,min_df=25,ngram_range=(1,2),token_pattern= '(\\S+)') #\n",
    "count_feature = cv.fit_transform(readme_list)\n",
    "print(count_feature.shape)\n",
    "print()\n",
    "print(count_feature)\n",
    "csv_filename = 'cv.text' #\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(str(cv.vocabulary_))\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重点主题不在特征词频则扩充矩阵\n",
    "import pandas as pd\n",
    "topic_list = pd.read_csv('main256_topic.csv')\n",
    "topics = list(topic_list['topics'])\n",
    "topic_order = []\n",
    "without_t = []\n",
    "for i in range(256):\n",
    "    if topics[i] not in cv.vocabulary_.keys():\n",
    "        print(i)\n",
    "        without_t.append(i)\n",
    "        topic_order.append(0)\n",
    "        continue\n",
    "    topic_order.append(cv.vocabulary_[topics[i]])\n",
    "for i in without_t:\n",
    "    print(topics[i])\n",
    "print(topic_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_order[235])\n",
    "topic_order[235] = count_feature.shape[1]\n",
    "print(topic_order[235])\n",
    "feature_arr = count_feature.toarray()\n",
    "print(feature_arr.shape)\n",
    "lst1 = np.array([[0]] * 12281)\n",
    "print(lst1.shape)\n",
    "count_feature = np.append(feature_arr,lst1,axis=1)\n",
    "print(count_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加上主题关联度特征\n",
    "txt_filename = 'feature_vector/feature_without.txt'\n",
    "with open(txt_filename,'r',encoding='utf-8') as f:\n",
    "    feature = eval(f.read())\n",
    "print(feature[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_feature.shape[0])\n",
    "feature = np.array(feature, dtype=np.float64)\n",
    "fea_add = []\n",
    "percent = 0.25 # 定义关联度信息占比\n",
    "for i in range(count_feature.shape[0]):\n",
    "    one_f = count_feature[i]\n",
    "    max_count = max(one_f)\n",
    "    max_f = max(feature[i])\n",
    "    mul_num = max_count*percent/max_f\n",
    "    fea_add.append(mul_num * feature[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_feature[0][topic_order[0]])\n",
    "print(topic_order[0])\n",
    "print(fea_add[0])\n",
    "print(len(fea_add[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按比例加入主题关联度特征\n",
    "x_feature = []\n",
    "count_feature_f = count_feature\n",
    "for i in range(count_feature.shape[0]):\n",
    "    new_feature = count_feature_f[i]\n",
    "    for j in range(256):\n",
    "        new_feature[topic_order[j]] += fea_add[i][j]\n",
    "    x_feature.append(new_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算词频矩阵的 tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transform = TfidfTransformer()\n",
    "tfidf_feature = transform.fit_transform(count_feature)\n",
    "print(tfidf_feature.shape)\n",
    "print()\n",
    "print(tfidf_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "txt_filename = 'y.txt'\n",
    "with open(txt_filename,'r',encoding='utf-8') as f:\n",
    "    y = eval(f.read())\n",
    "topic_num = []\n",
    "for i in range(len(y)):\n",
    "    topic_num.append(sum(y[i]))\n",
    "mode = statistics.mode(topic_num)\n",
    "median = statistics.mean(topic_num)\n",
    "mean = statistics.mean(topic_num)\n",
    "print(\"众数：\",mode)\n",
    "print(\"中位数：\",median)\n",
    "print(\"平均数：\",mean)\n",
    "y = np.array(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_feature, y, test_size=0.25, random_state = 50)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逻辑回归法\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn import metrics\n",
    "\n",
    "clf_multilabel = OneVsRestClassifier(LogisticRegression(solver='lbfgs',max_iter=300,C=0.9))\n",
    "\n",
    "clf_multilabel.fit(X_train,y_train) \n",
    "print(\"逻辑回归模型训练完毕！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = clf_multilabel.predict(X_test)  \n",
    "val_pred_proba = clf_multilabel.predict_proba(X_test)\n",
    "for i in range(len(y_test)):\n",
    "    one_pred = list(val_pred[i])\n",
    "    if sum(one_pred)==0:\n",
    "        one_pred_proba = list(val_pred_proba[i])\n",
    "        pred_proba = sorted(one_pred_proba)\n",
    "        val_pred[i][one_pred_proba.index(pred_proba[-1])] = 1\n",
    "        val_pred[i][one_pred_proba.index(pred_proba[-2])] = 1\n",
    "    elif sum(one_pred)==1:\n",
    "        one_pred_proba = list(val_pred_proba[i])\n",
    "        pred_proba = sorted(one_pred_proba)\n",
    "        val_pred[i][one_pred_proba.index(pred_proba[-2])] = 1\n",
    " \n",
    "p_score = metrics.precision_score(y_test,val_pred,average='micro')\n",
    "print(\"该方法精确度为：\",p_score)\n",
    "r_score = metrics.recall_score(y_test,val_pred,average='micro') # 被模型正确预测为正类的样本占所有实际为正类的样本的比例\n",
    "print(\"该方法召回率为：\",r_score)  \n",
    "F1_score = 2 * p_score * r_score / (p_score + r_score)\n",
    "print(\"该方法 F1 分数为：\",F1_score)\n",
    "β = 0.8\n",
    "Fβ_score = (1 + β * β) * p_score * r_score / (β * β * p_score + r_score)\n",
    "print(\"该方法 Fβ 分数为：\",Fβ_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 朴素贝叶斯法\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn import metrics\n",
    "\n",
    "clf_multilabel = OneVsRestClassifier(MultinomialNB()) \n",
    "\n",
    "clf_multilabel.fit(X_train,y_train)  \n",
    "print(\"朴素贝叶斯模型训练完毕！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = clf_multilabel.predict(X_test)\n",
    "val_pred_proba = clf_multilabel.predict_proba(X_test)\n",
    "for i in range(len(y_test)):\n",
    "    one_pred = list(val_pred[i])\n",
    "    if sum(one_pred)==0:\n",
    "        one_pred_proba = list(val_pred_proba[i])\n",
    "        pred_proba = sorted(one_pred_proba)\n",
    "        val_pred[i][one_pred_proba.index(pred_proba[-1])] = 1\n",
    "        val_pred[i][one_pred_proba.index(pred_proba[-2])] = 1\n",
    "    elif sum(one_pred)==1:\n",
    "        one_pred_proba = list(val_pred_proba[i])\n",
    "        pred_proba = sorted(one_pred_proba)\n",
    "        val_pred[i][one_pred_proba.index(pred_proba[-2])] = 1\n",
    " \n",
    "p_score = metrics.precision_score(y_test,val_pred,average='micro')\n",
    "print(\"该方法精确度为：\",p_score)\n",
    "r_score = metrics.recall_score(y_test,val_pred,average='micro') \n",
    "print(\"该方法召回率为：\",r_score)\n",
    "F1_score = 2 * p_score * r_score / (p_score + r_score)\n",
    "print(\"该方法 F1 分数为：\",F1_score)\n",
    "β = 0.8\n",
    "Fβ_score = (1 + β * β) * p_score * r_score / (β * β * p_score + r_score)\n",
    "print(\"该方法 Fβ 分数为：\",Fβ_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性支持向量机法\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn import metrics\n",
    "\n",
    "clf_multilabel = OneVsRestClassifier(LinearSVC()) \n",
    "\n",
    "clf_multilabel.fit(X_train,y_train) \n",
    "print(\"线性支持向量机模型训练完毕！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = clf_multilabel.predict(X_test) \n",
    "val_pred_proba = clf_multilabel.decision_function(X_test)\n",
    "for i in range(len(y_test)):\n",
    "    one_pred = list(val_pred[i])\n",
    "    if sum(one_pred)==0:\n",
    "        one_pred_proba = list(val_pred_proba[i])\n",
    "        pred_proba = sorted(one_pred_proba)\n",
    "        val_pred[i][one_pred_proba.index(pred_proba[-1])] = 1\n",
    "        val_pred[i][one_pred_proba.index(pred_proba[-2])] = 1\n",
    "    elif sum(one_pred)==1:\n",
    "        one_pred_proba = list(val_pred_proba[i])\n",
    "        pred_proba = sorted(one_pred_proba)\n",
    "        val_pred[i][one_pred_proba.index(pred_proba[-2])] = 1\n",
    " \n",
    "p_score = metrics.precision_score(y_test,val_pred,average='micro')\n",
    "print(\"该方法精确度为：\",p_score)\n",
    "r_score = metrics.recall_score(y_test,val_pred,average='micro') \n",
    "print(\"该方法召回率为：\",r_score) \n",
    "F1_score = 2 * p_score * r_score / (p_score + r_score)\n",
    "print(\"该方法 F1 分数为：\",F1_score)\n",
    "β = 0.8\n",
    "Fβ_score = (1 + β * β) * p_score * r_score / (β * β * p_score + r_score)\n",
    "print(\"该方法 Fβ 分数为：\",Fβ_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树法\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "clf_multilabel = OneVsRestClassifier(DecisionTreeClassifier(max_depth=15)) \n",
    "\n",
    "clf_multilabel.fit(X_train,y_train)\n",
    "print(\"决策树模型训练完毕！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = clf_multilabel.predict(X_test) \n",
    "val_pred_proba = clf_multilabel.predict_proba(X_test)\n",
    "for i in range(len(y_test)):\n",
    "    one_pred = list(val_pred[i])\n",
    "    if sum(one_pred)==0:\n",
    "        one_pred_proba = list(val_pred_proba[i])\n",
    "        pred_proba = sorted(one_pred_proba)\n",
    "        val_pred[i][one_pred_proba.index(pred_proba[-1])] = 1\n",
    "        val_pred[i][one_pred_proba.index(pred_proba[-2])] = 1\n",
    "    elif sum(one_pred)==1:\n",
    "        one_pred_proba = list(val_pred_proba[i])\n",
    "        pred_proba = sorted(one_pred_proba)\n",
    "        val_pred[i][one_pred_proba.index(pred_proba[-2])] = 1 \n",
    " \n",
    "p_score = metrics.precision_score(y_test,val_pred,average='micro')\n",
    "print(\"该方法精确度为：\",p_score)\n",
    "r_score = metrics.recall_score(y_test,val_pred,average='micro') \n",
    "print(\"该方法召回率为：\",r_score)  \n",
    "F1_score = 2 * p_score * r_score / (p_score + r_score)\n",
    "print(\"该方法 F1 分数为：\",F1_score)\n",
    "β = 0.8\n",
    "Fβ_score = (1 + β * β) * p_score * r_score / (β * β * p_score + r_score)\n",
    "print(\"该方法 Fβ 分数为：\",Fβ_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K 近邻法\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "clf_multilabel = OneVsRestClassifier(KNeighborsClassifier(n_neighbors=15)) \n",
    "\n",
    "clf_multilabel.fit(X_train,y_train) \n",
    "print(\"K 近邻模型训练完毕！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = clf_multilabel.predict(X_test)  \n",
    "val_pred_proba = clf_multilabel.predict_proba(X_test)\n",
    "for i in range(len(y_test)):\n",
    "    one_pred = list(val_pred[i])\n",
    "    if sum(one_pred)==0:\n",
    "        one_pred_proba = list(val_pred_proba[i])\n",
    "        pred_proba = sorted(one_pred_proba)\n",
    "        val_pred[i][one_pred_proba.index(pred_proba[-1])] = 1\n",
    "        val_pred[i][one_pred_proba.index(pred_proba[-2])] = 1\n",
    "    elif sum(one_pred)==1:\n",
    "        one_pred_proba = list(val_pred_proba[i])\n",
    "        pred_proba = sorted(one_pred_proba)\n",
    "        val_pred[i][one_pred_proba.index(pred_proba[-2])] = 1\n",
    " \n",
    "p_score = metrics.precision_score(y_test,val_pred,average='micro')\n",
    "print(\"该方法精确度为：\",p_score)\n",
    "r_score = metrics.recall_score(y_test,val_pred,average='micro') \n",
    "print(\"该方法召回率为：\",r_score)  \n",
    "F1_score = 2 * p_score * r_score / (p_score + r_score)\n",
    "print(\"该方法 F1 分数为：\",F1_score)\n",
    "β = 0.8\n",
    "Fβ_score = (1 + β * β) * p_score * r_score / (β * β * p_score + r_score)\n",
    "print(\"该方法 Fβ 分数为：\",Fβ_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
