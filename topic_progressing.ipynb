{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_csv = pd.read_csv('topics9.csv')\n",
    "topic_csv_len = len(topic_csv)\n",
    "print(\"主题数据量为：\", topic_csv_len)\n",
    "topics_list = topic_csv['topics']\n",
    "print(topics_list)\n",
    "topic_set = set()\n",
    "count = 0\n",
    "topic = []\n",
    "for i in range(topic_csv_len):\n",
    "    topic_list = set(eval(topics_list[i]))\n",
    "    topic.append(list(topic_list))\n",
    "    topic_set = topic_set.union(topic_list)\n",
    "    count += len(topic_list)\n",
    "print('标签数据预处理后不同标签数量：', len(topic_set))\n",
    "print('标签数据预处理后标签总数：', count)\n",
    "print('标签数据预处理后各标签平均出现频率：', round(count/len(topic_set),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建仓库标签字典\n",
    "topic_dict = dict()\n",
    "for i in range(topic_csv_len):\n",
    "    topic_list = eval(topics_list[i])\n",
    "    for item in topic_list:\n",
    "        if item not in topic_dict.keys():\n",
    "            topic_dict.update({item:1})\n",
    "        else:\n",
    "            topic_dict[item]+=1\n",
    "print(topic_dict)\n",
    "topic_dict = sorted(topic_dict.items(), key=lambda d:d[1], reverse=True)\n",
    "print(topic_dict)\n",
    "csv_filename = 'topic_dict.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topics,freq\\n')\n",
    "    for i in range(len(topic_dict)):\n",
    "        f.write(topic_dict[i][0]) \n",
    "        f.write(\",\")\n",
    "        f.write(str(topic_dict[i][1]))\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_csv = pd.read_csv('topic_dict.csv')\n",
    "topic_csv_len = len(topic_csv)\n",
    "print(\"主题数据量为：\", topic_csv_len)\n",
    "freq_list = topic_csv['freq']\n",
    "freq_dict = dict()\n",
    "for i in range(topic_csv_len):\n",
    "    item = freq_list[i]\n",
    "    if item not in freq_dict.keys():\n",
    "        freq_dict.update({item:1})\n",
    "    else:\n",
    "        freq_dict[item]+=1\n",
    "print(freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_range_dict = dict()\n",
    "key = list(freq_dict.keys())\n",
    "value = list(freq_dict.values())\n",
    "print(key)\n",
    "print(value)\n",
    "for i in range(len(freq_dict)):\n",
    "    if int(key[i])>500:\n",
    "        if '>500' not in freq_range_dict.keys():\n",
    "            freq_range_dict.update({'>500':value[i]})\n",
    "        else:\n",
    "            freq_range_dict['>500']+=value[i]\n",
    "    elif int(key[i])>200:\n",
    "        if '200~500' not in freq_range_dict.keys():\n",
    "            freq_range_dict.update({'200~500':value[i]})\n",
    "        else:\n",
    "            freq_range_dict['200~500']+=value[i]\n",
    "    elif int(key[i])>100:\n",
    "        if '100~200' not in freq_range_dict.keys():\n",
    "            freq_range_dict.update({'100~200':value[i]})\n",
    "        else:\n",
    "            freq_range_dict['100~200']+=value[i]\n",
    "    elif int(key[i])>60:\n",
    "        if '60~100' not in freq_range_dict.keys():\n",
    "            freq_range_dict.update({'60~100':value[i]})\n",
    "        else:\n",
    "            freq_range_dict['60~100']+=value[i]\n",
    "    elif int(key[i])>40:\n",
    "        if '40~60' not in freq_range_dict.keys():\n",
    "            freq_range_dict.update({'40~60':value[i]})\n",
    "        else:\n",
    "            freq_range_dict['40~60']+=value[i]\n",
    "    elif int(key[i])>27:\n",
    "        if '27~40' not in freq_range_dict.keys():\n",
    "            freq_range_dict.update({'27~40':value[i]})\n",
    "        else:\n",
    "            freq_range_dict['27~40']+=value[i]\n",
    "    elif int(key[i])>20:\n",
    "        if '20~27' not in freq_range_dict.keys():\n",
    "            freq_range_dict.update({'20~27':value[i]})\n",
    "        else:\n",
    "            freq_range_dict['20~27']+=value[i]\n",
    "    elif int(key[i])>15:\n",
    "        if '15~20' not in freq_range_dict.keys():\n",
    "            freq_range_dict.update({'15~20':value[i]})\n",
    "        else:\n",
    "            freq_range_dict['15~20']+=value[i]\n",
    "    else:\n",
    "        freq_range_dict.update({key[i]:value[i]})\n",
    "print(freq_range_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "\n",
    "key = list(reversed(list(freq_range_dict.keys())))\n",
    "value = list(reversed(list(freq_range_dict.values())))\n",
    "print(len(key),len(value))\n",
    "\n",
    "# 创建折线图\n",
    "fig, ax = plt.subplots(figsize=(14,5))\n",
    "ax.plot(key,value)\n",
    "ax.set_ylim(ymin=0, ymax=12000) # 设置y轴范围\n",
    " \n",
    "# 设置y轴主要刻度线\n",
    "ymajorLocator = MultipleLocator(2000)          # 将y轴主刻度标签设置为2000\n",
    "ymajorFormatter = FormatStrFormatter('%d')     # 设置y轴标签文本的格式\n",
    " \n",
    "ax.yaxis.set_major_locator(ymajorLocator)      # 设置y轴主刻度\n",
    "ax.yaxis.set_major_formatter(ymajorFormatter)  # 设置y轴标签文本格式\n",
    "ax.axhline(2000, color='grey', lw=2, alpha=0.2)\n",
    "# 设置y轴次要刻度线\n",
    "yminorLocator = MultipleLocator(200)           # 将此y轴次刻度标签设置为200\n",
    "ax.yaxis.set_minor_locator(yminorLocator)      # 设置y轴次刻度\n",
    "x = [i for i in range(0,23)]\n",
    "print(x)\n",
    "for a, b in zip(x,value):\n",
    "    plt.text(a, b+10, '%d' % b, horizontalalignment='center', verticalalignment='bottom', fontsize=10)\n",
    "plt.xlabel(\"Different topic tag frequencies\")\n",
    "plt.ylabel(\"Number of different topic tags\")\n",
    "plt.xticks(rotation=40)\n",
    "plt.savefig('figure.png',)\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据标签分节长度存储\n",
    "topic_1 = []\n",
    "topic_2 = []\n",
    "for item in topic_dict:\n",
    "    if len(item[0].split('-'))>1:\n",
    "        topic_2.append(item)\n",
    "    else:\n",
    "        topic_1.append(item)\n",
    "print(topic_2)\n",
    "print(topic_1)\n",
    "topic_1_len = len(topic_1)\n",
    "topic_2_len = len(topic_2)\n",
    "csv_filename = 'topic_dict_1.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topics,freq\\n')\n",
    "    for i in range(topic_1_len):\n",
    "        f.write(topic_1[i][0]) \n",
    "        f.write(\",\")\n",
    "        f.write(str(topic_1[i][1]))\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")\n",
    "csv_filename = 'topic_dict_2.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topics,freq\\n')\n",
    "    for i in range(topic_2_len):\n",
    "        f.write(topic_2[i][0]) \n",
    "        f.write(\",\")\n",
    "        f.write(str(topic_2[i][1]))\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标签按比例选取\n",
    "main_topic_num = 500\n",
    "topic_1_num = int(round(main_topic_num*topic_1_len/(topic_2_len+topic_1_len),0))\n",
    "topic_2_num = int(round(main_topic_num*topic_2_len/(topic_2_len+topic_1_len),0))\n",
    "print(topic_1_num,topic_2_num)\n",
    "main_topic = topic_1[:topic_1_num]\n",
    "main_topic = main_topic + (topic_2[:topic_2_num])\n",
    "print(main_topic)\n",
    "csv_filename = 'main500_topic.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topics,freq\\n')\n",
    "    for i in range(main_topic_num):\n",
    "        f.write(main_topic[i][0]) \n",
    "        f.write(\",\")\n",
    "        f.write(str(main_topic[i][1]))\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结合之前处理过程中的标签分布状况计算标签分数，避免切分不够好导致与用户标记标签意愿相左\n",
    "main500_topic_csv = pd.read_csv('main500_topic.csv')\n",
    "# 原主题 0.1，未拆解 0.2，最终标签 0.7\n",
    "origin_topic_csv = pd.read_csv('origin_topics.csv')\n",
    "split_topic_csv = pd.read_csv('topics4.csv')\n",
    "final_topic_csv = pd.read_csv('topics9.csv')\n",
    "topic_csv_len = len(origin_topic_csv)\n",
    "origin_dict = {}\n",
    "topics_list = origin_topic_csv['topics']\n",
    "for i in range(topic_csv_len):\n",
    "    one_topic_list = eval(topics_list[i])\n",
    "    for item in one_topic_list:\n",
    "        if item in list(main500_topic_csv['topics']):\n",
    "            if item not in list(origin_dict.keys()):\n",
    "                origin_dict.update({item:1})\n",
    "            else:\n",
    "                origin_dict[item] += 1\n",
    "print(len(origin_dict))\n",
    "split_dict = {}\n",
    "topics_list = split_topic_csv['topics']\n",
    "for i in range(topic_csv_len):\n",
    "    one_topic_list = eval(topics_list[i])\n",
    "    for item in one_topic_list:\n",
    "        if item in list(main500_topic_csv['topics']):\n",
    "            if item not in list(split_dict.keys()):\n",
    "                split_dict.update({item:1})\n",
    "            else:\n",
    "                split_dict[item] += 1\n",
    "print(len(split_dict))\n",
    "final_dict = {}\n",
    "topics_list = final_topic_csv['topics']\n",
    "for i in range(topic_csv_len):\n",
    "    one_topic_list = eval(topics_list[i])\n",
    "    for item in one_topic_list:\n",
    "        if item in list(main500_topic_csv['topics']):\n",
    "            if item not in list(final_dict.keys()):\n",
    "                final_dict.update({item:1})\n",
    "            else:\n",
    "                final_dict[item] += 1\n",
    "print(len(final_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除原始主题不存在标签进入重点主题集资格\n",
    "print(origin_dict)\n",
    "topic_score = {}\n",
    "for item in list(origin_dict.keys()):\n",
    "    topic_score[item] = round(0.1 * origin_dict[item] + 0.2 * split_dict[item] + 0.7 * final_dict[item],2)\n",
    "print(topic_score)\n",
    "topic_score = sorted(topic_score.items(), key=lambda d:d[1], reverse=True)\n",
    "print(topic_score)\n",
    "csv_filename = 'main500_topic_score.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topics,freq\\n')\n",
    "    for i in range(len(origin_dict)):\n",
    "        f.write(topic_score[i][0])\n",
    "        f.write(\",\")\n",
    "        f.write(str(topic_score[i][1]))\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选取前 256 个主要标签，用于后续展示网络图\n",
    "main_topic_csv = pd.read_csv('main500_topic_score.csv')\n",
    "topic_256 = main_topic_csv.iloc[0:256]\n",
    "topic_256.to_csv(\"main256_topic.csv\",index=0)\n",
    "print(topic_256)\n",
    "main_topics_list = list(topic_256['topics'])\n",
    "print(main_topics_list)\n",
    "repo_topic_csv = pd.read_csv('topics9.csv')\n",
    "repo_topic_lists = repo_topic_csv['topics']\n",
    "repo_len = len(repo_topic_lists)\n",
    "only_main_topics = []\n",
    "for i in range(repo_len):\n",
    "    one_repo = eval(repo_topic_lists[i])\n",
    "    one_only_main = []\n",
    "    for item in one_repo:\n",
    "        if item in main_topics_list:\n",
    "            one_only_main.append(item)\n",
    "    only_main_topics.append(one_only_main)\n",
    "print(only_main_topics)\n",
    "csv_filename = 'topics_only_main.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topics\\n')\n",
    "    for i in range(repo_len):\n",
    "        f.write(\"\\\"\")\n",
    "        f.write(str(only_main_topics[i])) \n",
    "        f.write(\"\\\"\")\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标签数据处理完后还要再次对标签量小于 2 的仓库数据进行去除\n",
    "repo_topic_csv = pd.read_csv('topics_only_main.csv')\n",
    "repo_topic_lists = repo_topic_csv['topics']\n",
    "repo_len = len(repo_topic_lists)\n",
    "main2_topics = []\n",
    "for i in range(repo_len):\n",
    "    one_repo = eval(repo_topic_lists[i])\n",
    "    if len(one_repo)>1:\n",
    "        main2_topics.append(i)\n",
    "print(repo_topic_csv.iloc[main2_topics])\n",
    "repo_topic_csv.iloc[main2_topics].to_csv('final_topics.csv',index=0)\n",
    "repo_infor_csv = pd.read_csv('data_new3.csv')\n",
    "repo_infor_csv.iloc[main2_topics].to_csv('final_infors.csv',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建边与节点关系\n",
    "# 以边频为边权重\n",
    "repo_topic_csv = pd.read_csv('final_topics.csv')\n",
    "repo_topic_lists = repo_topic_csv['topics']\n",
    "repo_len = len(repo_topic_csv)\n",
    "link_weight = dict()\n",
    "for i in range(repo_len):\n",
    "    one_repo = eval(repo_topic_lists[i])\n",
    "    one_len = len(one_repo)\n",
    "    for j in range(one_len-1):\n",
    "        for k in range(j+1,one_len):\n",
    "            # one_repo[j],one_repo[k]建立一条连边\n",
    "            label_link = one_repo[j]+','+one_repo[k]\n",
    "            label_link_reverse = one_repo[k]+','+one_repo[j]\n",
    "            if (label_link not in list(link_weight.keys())) and (label_link_reverse not in list(link_weight.keys())):\n",
    "                link_weight.update({label_link:1}) # 以边频为边权重\n",
    "            else:\n",
    "                if (label_link in list(link_weight.keys())):\n",
    "                    link_weight[label_link] += 1\n",
    "                else:\n",
    "                    link_weight[label_link_reverse] += 1\n",
    "print(link_weight)\n",
    "csv_filename = 'link_weight.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topic1,topic2,freq\\n')\n",
    "    for i in range(len(link_weight)):\n",
    "        f.write(list(link_weight.keys())[i]) \n",
    "        f.write(\",\")\n",
    "        f.write(str(list(link_weight.values())[i]))\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 边频为边权重的对角线\n",
    "repo_topic_csv = pd.read_csv('final_topics.csv')\n",
    "repo_topic_lists = repo_topic_csv['topics']\n",
    "repo_len = len(repo_topic_csv)\n",
    "topic_csv = pd.read_csv('main256_topic.csv')\n",
    "topics_order = list(topic_csv['topics'])\n",
    "\n",
    "diagonal_topic = dict()\n",
    "for i in range(repo_len):\n",
    "    one_repo = eval(repo_topic_lists[i])\n",
    "    for item in one_repo:\n",
    "        if item not in diagonal_topic.keys():\n",
    "            diagonal_topic.update({item:1})\n",
    "        else:\n",
    "            diagonal_topic[item]+=1\n",
    "print(diagonal_topic)\n",
    "\n",
    "diagonal_num = []\n",
    "for item in topics_order:\n",
    "    diagonal_num.append(diagonal_topic[item])\n",
    "print(len(diagonal_num))\n",
    "\n",
    "txt_filename = 'topic_matrix/diagonal_num.txt'\n",
    "with open(txt_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(str(diagonal_num))\n",
    "    f.write('\\n')\n",
    "print(f\"Data saved to {txt_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "topic_csv = pd.read_csv('main256_topic.csv')\n",
    "topics_order = list(topic_csv['topics'])\n",
    "topics_score = list(topic_csv['freq'])\n",
    "print(topics_order)\n",
    "print(topics_score)\n",
    "with open('topic_matrix/diagonal_num.txt', 'r', encoding='utf-8') as f:\n",
    "    readme_csv = f.read()\n",
    "print(\"文件加载完毕\")\n",
    "readme_list = eval(readme_csv)\n",
    "print(readme_list)\n",
    "readme_csv_len = len(readme_list)\n",
    "print(\"重点主题集大小为：\",readme_csv_len)\n",
    "print(min(readme_list),sum(readme_list))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "x1 = [i-0.16 for i in range(0,50)]\n",
    "x2 = [i+0.16 for i in range(0,50)]\n",
    "ax.bar(x1,topics_score[:50],width=0.32,label='topic score')\n",
    "ax.bar(x2,readme_list[:50],width=0.32,label='topics frequency') \n",
    "# 设置y轴范围\n",
    "ax.set_ylim(ymin=0, ymax=1700)\n",
    " \n",
    "# 设置y轴主要刻度线\n",
    "ymajorLocator = MultipleLocator(400)       \n",
    "ymajorFormatter = FormatStrFormatter('%d') \n",
    " \n",
    "ax.yaxis.set_major_locator(ymajorLocator)      \n",
    "ax.yaxis.set_major_formatter(ymajorFormatter)  \n",
    "ax.axhline(500, color='grey', lw=2, alpha=0.2)\n",
    "# 设置y轴次要刻度线\n",
    "yminorLocator = MultipleLocator(100)         \n",
    "ax.yaxis.set_minor_locator(yminorLocator)   \n",
    " \n",
    " \n",
    "plt.xlabel(\"Important Topics Top 50\")\n",
    "plt.ylabel(\"Topic Score & Topic Frequency\")\n",
    "plt.xticks(range(50),topics_order[:50],rotation=90)\n",
    "\n",
    "plt.legend() \n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以 log10(边频)+1 为边权重\n",
    "link_csv = pd.read_csv('link_weight.csv')\n",
    "link_freq = link_csv['freq']\n",
    "link_len = len(link_csv)\n",
    "link_score = []\n",
    "for i in range(link_len):\n",
    "    link_score.append(math.log10(link_freq[i])+1)\n",
    "csv_filename = 'link_weight_log.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topic1,topic2,log_freq\\n')\n",
    "    for i in range(link_len):\n",
    "        f.write(link_csv['topic1'][i]) \n",
    "        f.write(\",\")\n",
    "        f.write(link_csv['topic2'][i]) \n",
    "        f.write(\",\")\n",
    "        f.write(str(link_score[i]))\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log10(边频)+1 为边权重的对角线\n",
    "txt_filename = 'diagonal_num.txt'\n",
    "with open(txt_filename, 'r', encoding='utf-8') as f:\n",
    "    count_num = eval(str(f.read()))\n",
    "    diagonal_num_log = []\n",
    "    for item in count_num:\n",
    "        diagonal_num_log.append(math.log10(item)+1)\n",
    "print(diagonal_num_log)\n",
    "txt_filename = 'topic_matrix/diagonal_num_log.txt'\n",
    "with open(txt_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(str(diagonal_num_log))\n",
    "    f.write('\\n')\n",
    "print(f\"Data saved to {txt_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建边与节点关系\n",
    "# 以合作分数为边权重\n",
    "repo_topic_csv = pd.read_csv('final_topics.csv')\n",
    "repo_topic_lists = repo_topic_csv['topics']\n",
    "repo_len = len(repo_topic_csv)\n",
    "link_weight = dict()\n",
    "for i in range(repo_len):\n",
    "    one_repo = eval(repo_topic_lists[i])\n",
    "    one_len = len(one_repo)\n",
    "    for j in range(one_len-1):\n",
    "        for k in range(j+1,one_len):\n",
    "            # one_repo[j],one_repo[k]建立一条连边\n",
    "            label_link = one_repo[j]+','+one_repo[k]\n",
    "            label_link_reverse = one_repo[k]+','+one_repo[j]\n",
    "            if (label_link not in list(link_weight.keys())) and (label_link_reverse not in list(link_weight.keys())):\n",
    "                link_weight.update({label_link:(1/(one_len-1))}) # 以合作分数为边权重\n",
    "            else:\n",
    "                if (label_link in list(link_weight.keys())):\n",
    "                    link_weight[label_link] += (1/(one_len-1))\n",
    "                else:\n",
    "                    link_weight[label_link_reverse] += (1/(one_len-1))\n",
    "print(link_weight)\n",
    "csv_filename = 'link_weight_collaborate.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topic1,topic2,collaborate_freq\\n')\n",
    "    for i in range(len(link_weight)):\n",
    "        f.write(list(link_weight.keys())[i]) \n",
    "        f.write(\",\")\n",
    "        f.write(str(list(link_weight.values())[i]))\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合作分数为边权重的对角线\n",
    "repo_topic_csv = pd.read_csv('final_topics.csv')\n",
    "repo_topic_lists = repo_topic_csv['topics']\n",
    "repo_len = len(repo_topic_csv)\n",
    "topic_csv = pd.read_csv('main256_topic.csv')\n",
    "topics_order = list(topic_csv['topics'])\n",
    "\n",
    "diagonal_topic = dict()\n",
    "for i in range(repo_len):\n",
    "    one_repo = eval(repo_topic_lists[i])\n",
    "    one_len = len(one_repo)\n",
    "    for item in one_repo:\n",
    "        if item not in diagonal_topic.keys():\n",
    "            diagonal_topic.update({item:(1/(one_len))})\n",
    "        else:\n",
    "            diagonal_topic[item]+=(1/(one_len))\n",
    "print(diagonal_topic)\n",
    "\n",
    "diagonal_num = []\n",
    "for item in topics_order:\n",
    "    diagonal_num.append(diagonal_topic[item])\n",
    "print(len(diagonal_num))\n",
    "\n",
    "txt_filename = 'topic_matrix/diagonal_num_collaborate.txt'\n",
    "with open(txt_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(str(diagonal_num))\n",
    "    f.write('\\n')\n",
    "print(f\"Data saved to {txt_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检测对角线取值合理\n",
    "repo_topic_csv = pd.read_csv('link_weight_collaborate.csv')\n",
    "topic1 = list(repo_topic_csv['topic1'])\n",
    "topic2 = list(repo_topic_csv['topic2'])\n",
    "repo_len = len(repo_topic_csv)\n",
    "print(repo_len)\n",
    "total = 0\n",
    "for i in range(repo_len):\n",
    "    if topic1[i]=='interview' or topic2[i]=='interview':\n",
    "        total += repo_topic_csv['collaborate_freq'][i]\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结合仓库收藏量设置边权重\n",
    "repo_csv = pd.read_csv('final_infors.csv')\n",
    "repo_star = list(repo_csv['stars'])\n",
    "repo_len = len(repo_star)\n",
    "star_10000 = []\n",
    "star_6000 = []\n",
    "star_3000 = []\n",
    "star_2000 = [] \n",
    "for i in range(repo_len):\n",
    "    if repo_star[i]>=10000:\n",
    "        star_10000.append(i)\n",
    "    elif repo_star[i]>=6000:\n",
    "        star_6000.append(i)\n",
    "    elif repo_star[i]>=3000:\n",
    "        star_3000.append(i)\n",
    "    else:\n",
    "        star_2000.append(i)\n",
    "repo_topic_csv = pd.read_csv('final_topics.csv')\n",
    "repo_topic_lists = repo_topic_csv['topics']\n",
    "repo_len = len(repo_topic_csv)\n",
    "link_weight = dict()\n",
    "for i in star_10000:\n",
    "    one_repo = eval(repo_topic_lists[i])\n",
    "    one_len = len(one_repo)\n",
    "    for j in range(one_len-1):\n",
    "        for k in range(j+1,one_len):\n",
    "            # one_repo[j],one_repo[k]建立一条连边\n",
    "            label_link = one_repo[j]+','+one_repo[k]\n",
    "            label_link_reverse = one_repo[k]+','+one_repo[j]\n",
    "            if (label_link not in list(link_weight.keys())) and (label_link_reverse not in list(link_weight.keys())):\n",
    "                link_weight.update({label_link:1.25}) # 以收藏量分数为边权重\n",
    "            else:\n",
    "                if (label_link in list(link_weight.keys())):\n",
    "                    link_weight[label_link] += 1.25\n",
    "                else:\n",
    "                    link_weight[label_link_reverse] += 1.25\n",
    "for i in star_6000:\n",
    "    one_repo = eval(repo_topic_lists[i])\n",
    "    one_len = len(one_repo)\n",
    "    for j in range(one_len-1):\n",
    "        for k in range(j+1,one_len):\n",
    "            # one_repo[j],one_repo[k]建立一条连边\n",
    "            label_link = one_repo[j]+','+one_repo[k]\n",
    "            label_link_reverse = one_repo[k]+','+one_repo[j]\n",
    "            if (label_link not in list(link_weight.keys())) and (label_link_reverse not in list(link_weight.keys())):\n",
    "                link_weight.update({label_link:1.1}) # 以收藏量分数为边权重\n",
    "            else:\n",
    "                if (label_link in list(link_weight.keys())):\n",
    "                    link_weight[label_link] += 1.1\n",
    "                else:\n",
    "                    link_weight[label_link_reverse] += 1.1\n",
    "for i in star_3000:\n",
    "    one_repo = eval(repo_topic_lists[i])\n",
    "    one_len = len(one_repo)\n",
    "    for j in range(one_len-1):\n",
    "        for k in range(j+1,one_len):\n",
    "            # one_repo[j],one_repo[k]建立一条连边\n",
    "            label_link = one_repo[j]+','+one_repo[k]\n",
    "            label_link_reverse = one_repo[k]+','+one_repo[j]\n",
    "            if (label_link not in list(link_weight.keys())) and (label_link_reverse not in list(link_weight.keys())):\n",
    "                link_weight.update({label_link:1}) # 以收藏量分数为边权重\n",
    "            else:\n",
    "                if (label_link in list(link_weight.keys())):\n",
    "                    link_weight[label_link] += 1\n",
    "                else:\n",
    "                    link_weight[label_link_reverse] += 1\n",
    "for i in star_2000:\n",
    "    one_repo = eval(repo_topic_lists[i])\n",
    "    one_len = len(one_repo)\n",
    "    for j in range(one_len-1):\n",
    "        for k in range(j+1,one_len):\n",
    "            # one_repo[j],one_repo[k]建立一条连边\n",
    "            label_link = one_repo[j]+','+one_repo[k]\n",
    "            label_link_reverse = one_repo[k]+','+one_repo[j]\n",
    "            if (label_link not in list(link_weight.keys())) and (label_link_reverse not in list(link_weight.keys())):\n",
    "                link_weight.update({label_link:0.8}) # 以收藏量分数为边权重\n",
    "            else:\n",
    "                if (label_link in list(link_weight.keys())):\n",
    "                    link_weight[label_link] += 0.8\n",
    "                else:\n",
    "                    link_weight[label_link_reverse] += 0.8\n",
    "print(link_weight)\n",
    "csv_filename = 'link_weight_star.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topic1,topic2,star_freq\\n')\n",
    "    for i in range(len(link_weight)):\n",
    "        f.write(list(link_weight.keys())[i]) \n",
    "        f.write(\",\")\n",
    "        f.write(str(round(list(link_weight.values())[i],2)))\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 收藏量分数为权重的对角线\n",
    "repo_topic_csv = pd.read_csv('final_topics.csv')\n",
    "repo_topic_lists = repo_topic_csv['topics']\n",
    "repo_len = len(repo_topic_csv)\n",
    "topic_csv = pd.read_csv('main256_topic.csv')\n",
    "topics_order = list(topic_csv['topics'])\n",
    "different_star = [star_10000,star_6000,star_3000,star_2000]\n",
    "star_num = [1.25,1.1,1,0.8]\n",
    "diagonal_topic = dict()\n",
    "for j in range(4):\n",
    "    for i in different_star[j]:\n",
    "        one_repo = eval(repo_topic_lists[i])\n",
    "        for item in one_repo:\n",
    "            if item not in diagonal_topic.keys():\n",
    "                diagonal_topic.update({item:star_num[j]})\n",
    "            else:\n",
    "                diagonal_topic[item]+=star_num[j]\n",
    "print(diagonal_topic)\n",
    "\n",
    "diagonal_num = []\n",
    "for item in topics_order:\n",
    "    diagonal_num.append(round(diagonal_topic[item],2))\n",
    "print(len(diagonal_num))\n",
    "\n",
    "txt_filename = 'topic_matrix/diagonal_num_star.txt'\n",
    "with open(txt_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(str(diagonal_num))\n",
    "    f.write('\\n')\n",
    "print(f\"Data saved to {txt_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分层次设置边权重\n",
    "## 其实好像和 log10 的方法一个意思，都是把直接计数造成的差倍缩小，只不过 log 按比例一一细分，分层按层次阶段分\n",
    "link_csv = pd.read_csv('link_weight.csv')\n",
    "link_freq = link_csv['freq']\n",
    "link_len = len(link_csv)\n",
    "standard1 = np.percentile(link_freq,70)\n",
    "standard2 = np.percentile(link_freq,90)\n",
    "standard3 = np.percentile(link_freq,95)\n",
    "standard4 = np.percentile(link_freq,98)\n",
    "standard5 = np.percentile(link_freq,99.5)\n",
    "link_score = []\n",
    "for i in range(link_len):\n",
    "    if link_freq[i] <= standard1:\n",
    "        link_score.append(1)\n",
    "    elif link_freq[i] <= standard2:\n",
    "        link_score.append(2)\n",
    "    elif link_freq[i] <= standard3:\n",
    "        link_score.append(3)\n",
    "    elif link_freq[i] <= standard4:\n",
    "        link_score.append(4)\n",
    "    elif link_freq[i] <= standard5:\n",
    "        link_score.append(5)\n",
    "    else:\n",
    "        link_score.append(6)\n",
    "print(link_len)\n",
    "csv_filename = 'link_weight_layer.csv'\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('topic1,topic2,layer_freq\\n')\n",
    "    for i in range(link_len):\n",
    "        f.write(link_csv['topic1'][i]) \n",
    "        f.write(\",\")\n",
    "        f.write(link_csv['topic2'][i]) \n",
    "        f.write(\",\")\n",
    "        f.write(str(link_score[i]))\n",
    "        f.write('\\n')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 层次为权重的对角线\n",
    "link_csv = pd.read_csv('link_weight.csv')\n",
    "link_freq = link_csv['freq']\n",
    "txt_filename = 'diagonal_num.txt'\n",
    "with open(txt_filename, 'r', encoding='utf-8') as f:\n",
    "    count_num = eval(str(f.read()))\n",
    "    diagonal_num_layer = []\n",
    "    for item in count_num:\n",
    "        if item <= np.percentile(link_freq,99.5):\n",
    "            diagonal_num_layer.append(5+0.5)\n",
    "        elif item <= 150:\n",
    "            diagonal_num_layer.append(6+0.5)\n",
    "        elif item <= max(link_freq):\n",
    "            diagonal_num_layer.append(7)\n",
    "        else:\n",
    "            diagonal_num_layer.append(7.5)\n",
    "print(len(diagonal_num_layer))\n",
    "txt_filename = 'topic_matrix/diagonal_num_layer.txt'\n",
    "with open(txt_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(str(diagonal_num_layer))\n",
    "    f.write('\\n')\n",
    "print(f\"Data saved to {txt_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodePen 上可视化数据处理\n",
    "import networkx as nx\n",
    "link_csv = pd.read_csv('visual_data/visial_link_weight2_star.csv') ##\n",
    "G = nx.read_weighted_edgelist('visual_data/visial_link_weight2_star.csv', delimiter=',', create_using=nx.Graph()) ##\n",
    "node_sizes = [d * 30 for _, d in G.degree()]\n",
    "degree_list = list(G.degree())\n",
    "print(degree_list)\n",
    "csv_filename = 'visual_data/node_infor_star.csv' ##\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(degree_list)):\n",
    "        f.write('{name:\\'')\n",
    "        f.write(degree_list[i][0])\n",
    "        f.write(\"\\',symbolSize:\")\n",
    "        f.write(str(10*math.log2(degree_list[i][1])))\n",
    "        f.write(\",value:\")\n",
    "        f.write(str(degree_list[i][1])) ##\n",
    "        f.write('},')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")\n",
    "csv_filename = 'visual_data/edge_infor_star.csv' ##\n",
    "with open(csv_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write('{source:\\'nodejs\\',target:\\'javascript\\',value:289.9}') ##\n",
    "    for i in range(len(link_csv)):\n",
    "        f.write(',{source:\\'')\n",
    "        f.write(link_csv['nodejs'][i])\n",
    "        f.write(\"\\',target:\\'\")\n",
    "        f.write(link_csv['javascript'][i])\n",
    "        f.write(\"\\',value:\")\n",
    "        f.write(str(round(float(link_csv['289.9'][i]),4))) ##\n",
    "        f.write('}')\n",
    "print(f\"Data saved to {csv_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 构建关联度矩阵\n",
    "## 将单词组标签换成对应索引\n",
    "link_csv = pd.read_csv('link_weight_star.csv') ##\n",
    "order_csv = pd.read_csv('main256_topic.csv')\n",
    "topic_order = order_csv['topics']\n",
    "link_len = len(link_csv)\n",
    "for i in range(len(order_csv)):\n",
    "    topic = topic_order[i]\n",
    "    print(\"修改标签\",topic,'为其索引')\n",
    "    for j in range(link_len):\n",
    "        if link_csv['topic1'][j] == topic:\n",
    "            link_csv['topic1'][j] = i\n",
    "        elif link_csv['topic2'][j] == topic:\n",
    "            link_csv['topic2'][j] = i    \n",
    "print(link_csv)\n",
    "link_csv.to_csv('topic_matrix/index_link_weight_star.csv',index=0) ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建 256*256 的主题关联度矩阵\n",
    "link_csv = pd.read_csv('topic_matrix/index_link_weight_collaborate.csv') ##\n",
    "topic_matrix = [[0]*256 for i in range(256)]\n",
    "for i in range(len(link_csv)):\n",
    "    topic_matrix[link_csv['topic1'][i]][link_csv['topic2'][i]] = round(link_csv['collaborate_freq'][i],4) #\n",
    "    topic_matrix[link_csv['topic2'][i]][link_csv['topic1'][i]] = round(link_csv['collaborate_freq'][i],4) #\n",
    "txt_filename = 'topic_matrix/diagonal_num_collaborate.txt' ##\n",
    "with open(txt_filename, 'r', encoding='utf-8') as f:\n",
    "    diagonal = eval(str(f.read()))\n",
    "    for i in range(len(diagonal)):\n",
    "        topic_matrix[i][i] = round(diagonal[i],4)\n",
    "txt_filename = 'topic_matrix/topic_matrix_collaborate.txt' ##\n",
    "with open(txt_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(str(topic_matrix))\n",
    "    f.write('\\n')\n",
    "print(f\"Data saved to {txt_filename} successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
